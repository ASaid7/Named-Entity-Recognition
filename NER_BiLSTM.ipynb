{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import gensim\n",
    "from string import punctuation\n",
    "import regex as re\n",
    "import random\n",
    "import time\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define word2vec embeddings and vocab dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.expanduser('~'), 'Documents', 'AIT 726','HW3')\n",
    "googlePath = os.path.join(os.path.expanduser('~'), 'Documents', 'AIT 726','HW3','GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "google = gensim.models.KeyedVectors.load_word2vec_format(googlePath,binary=True)\n",
    "\n",
    "idx2word = {idx: word for idx, word in enumerate(google.index2word)}\n",
    "word2idx = {word: idx for idx, word in enumerate(google.index2word)}\n",
    "embeddings = nn.Embedding.from_pretrained(torch.Tensor(google.vectors))\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read text file, create padding function for mini batching the data into the lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path):\n",
    "    f = open(path)\n",
    "    data = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]=='\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence,label))\n",
    "                sentence = []\n",
    "                label = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append(splits[0])\n",
    "        label.append(splits[-1][:-1])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        sentence = []\n",
    "        label = []\n",
    "    return data\n",
    "\n",
    "\n",
    "def padding(document):\n",
    "    _max = max([len(document[i][0]) for i in range(len(document))])\n",
    "    for i in range(len(document)):\n",
    "        document[i] = (document[i][0] + ['<PAD>']*(_max-len(document[i][0])),\n",
    "                document[i][1] + ['<PAD>']*(_max-len(document[i][1])))\n",
    "    return document\n",
    "\n",
    "\n",
    "def encoding(document):\n",
    "    vocab = []\n",
    "    tag = []\n",
    "    data = []\n",
    "    document = padding(document)\n",
    "    for i in range(len(document)):\n",
    "        vocab = torch.LongTensor([word2idx[j] if j in word2idx.keys() and j != '<PAD>' else\n",
    "                                       word2idx['unk'] for j in document[i][0]])\n",
    "        tag = torch.LongTensor([tag2idx[k] if j in word2idx.keys() or j != '<PAD>' else\n",
    "                                     tag2idx['<PAD>'] for j, k in zip(document[i][0], document[i][1])])\n",
    "        data.append((vocab,tag))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss function to not learn paddings and custom prediction function to not incorporate padding into the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    labels = labels.view(-1)\n",
    "    mask = (labels >= 1).float()\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "    return -torch.sum(outputs)/num_tokens\n",
    "\n",
    "def predict(outputs, labels):\n",
    "    labels = labels.view(-1)\n",
    "    outputs = outputs[labels!=0]\n",
    "    labels = labels[labels!=0]\n",
    "    preds = torch.max(outputs, 1)[1]\n",
    "    correct = torch.sum(preds==labels).type(torch.float)\n",
    "    return preds, correct\n",
    "    \n",
    "\n",
    "def trainModel(model, optimizer, dataset, minibatchsize, epoch):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = model.state_dict()\n",
    "    bestAcc = 0.0\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print('Epoch {}/{}'.format(i, epoch-1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            random.shuffle(dataset[phase])\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "\n",
    "            for j in range(0,len(dataset[phase]), minibatchsize):\n",
    "                z = encoding(dataset[phase][j:j+minibatchsize])\n",
    "                inputs, labels = zip(*z)\n",
    "                inputs = torch.stack(inputs)\n",
    "                labels = torch.stack(labels)\n",
    "                if use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                #Forward Pass\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                preds, correct = predict(outputs, labels)\n",
    "                #backward and optimization\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_correct += correct\n",
    "\n",
    "            epochLoss = total_loss\n",
    "            epochAcc = total_correct/datasetSize[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epochLoss, epochAcc))\n",
    "            \n",
    "            if phase == 'val' and epochAcc > bestAcc:\n",
    "                bestAcc = epochAcc\n",
    "                best_model_wts = model.state_dict()\n",
    "            \n",
    "        timeElapsed = time.time() - since\n",
    "        print('Elapsed {:.0f}m {:.0f}s\\n'.format(timeElapsed // 60, timeElapsed % 60))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(bestAcc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = readfile(path + '/train.txt')\n",
    "val = readfile(path+'/valid.txt')\n",
    "test = readfile(path+'/test.txt')\n",
    "n_t = sum([len(train[i][0]) for i in range(len(train))])\n",
    "n_v = sum([len(val[i][0]) for i in range(len(val))])\n",
    "n_ts = sum([len(test[i][0]) for i in range(len(test))])\n",
    "\n",
    "dataset = {'train': train,\n",
    "           'val': val,\n",
    "           'test': test}\n",
    "datasetSize = {'train': n_t,\n",
    "               'val': n_v,\n",
    "               'test': n_ts}\n",
    "del train, val, n_t, n_v # free memory\n",
    "tags = ('<PAD>', 'O', 'I-LOC', 'B-PER', 'I-PER', 'I-ORG','I-MISC','B-MISC', 'B-LOC', 'B-ORG')\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 77.8869 Acc: 0.8243\n",
      "val Loss: 17.1299 Acc: 0.8325\n",
      "Elapsed 0m 23s\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 73.0479 Acc: 0.8328\n",
      "val Loss: 17.0553 Acc: 0.8325\n",
      "Elapsed 0m 45s\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 56.9657 Acc: 0.8429\n",
      "val Loss: 8.7703 Acc: 0.8668\n",
      "Elapsed 1m 8s\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 34.2721 Acc: 0.8844\n",
      "val Loss: 7.1976 Acc: 0.8992\n",
      "Elapsed 1m 31s\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 28.9880 Acc: 0.9037\n",
      "val Loss: 6.3993 Acc: 0.9091\n",
      "Elapsed 1m 54s\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 26.7014 Acc: 0.9123\n",
      "val Loss: 6.2545 Acc: 0.9127\n",
      "Elapsed 2m 17s\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 24.8424 Acc: 0.9195\n",
      "val Loss: 5.6813 Acc: 0.9269\n",
      "Elapsed 2m 39s\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 23.1549 Acc: 0.9282\n",
      "val Loss: 5.4380 Acc: 0.9318\n",
      "Elapsed 3m 2s\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 21.4693 Acc: 0.9363\n",
      "val Loss: 4.9989 Acc: 0.9384\n",
      "Elapsed 3m 25s\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 20.1950 Acc: 0.9405\n",
      "val Loss: 4.8758 Acc: 0.9404\n",
      "Elapsed 3m 48s\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 19.4525 Acc: 0.9438\n",
      "val Loss: 4.7485 Acc: 0.9424\n",
      "Elapsed 4m 11s\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 19.0102 Acc: 0.9456\n",
      "val Loss: 4.7584 Acc: 0.9429\n",
      "Elapsed 4m 34s\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 18.2541 Acc: 0.9489\n",
      "val Loss: 4.5971 Acc: 0.9456\n",
      "Elapsed 4m 57s\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 17.6849 Acc: 0.9506\n",
      "val Loss: 4.6063 Acc: 0.9455\n",
      "Elapsed 5m 20s\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 17.0967 Acc: 0.9523\n",
      "val Loss: 4.4527 Acc: 0.9474\n",
      "Elapsed 5m 43s\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 16.7807 Acc: 0.9536\n",
      "val Loss: 4.4088 Acc: 0.9477\n",
      "Elapsed 6m 6s\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 16.3742 Acc: 0.9551\n",
      "val Loss: 4.4084 Acc: 0.9481\n",
      "Elapsed 6m 28s\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 16.0652 Acc: 0.9560\n",
      "val Loss: 4.3892 Acc: 0.9479\n",
      "Elapsed 6m 51s\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 16.0230 Acc: 0.9562\n",
      "val Loss: 4.3969 Acc: 0.9495\n",
      "Elapsed 7m 14s\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 15.5991 Acc: 0.9577\n",
      "val Loss: 4.3508 Acc: 0.9475\n",
      "Elapsed 7m 37s\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 15.3714 Acc: 0.9581\n",
      "val Loss: 4.2871 Acc: 0.9502\n",
      "Elapsed 7m 60s\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 15.0474 Acc: 0.9592\n",
      "val Loss: 4.3098 Acc: 0.9495\n",
      "Elapsed 8m 22s\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 14.8120 Acc: 0.9597\n",
      "val Loss: 4.2853 Acc: 0.9492\n",
      "Elapsed 8m 45s\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 14.7028 Acc: 0.9599\n",
      "val Loss: 4.2833 Acc: 0.9500\n",
      "Elapsed 9m 8s\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 14.5286 Acc: 0.9610\n",
      "val Loss: 4.2941 Acc: 0.9506\n",
      "Elapsed 9m 31s\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 14.3930 Acc: 0.9610\n",
      "val Loss: 4.3096 Acc: 0.9511\n",
      "Elapsed 9m 54s\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 14.1232 Acc: 0.9620\n",
      "val Loss: 4.2464 Acc: 0.9510\n",
      "Elapsed 10m 17s\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 13.9516 Acc: 0.9622\n",
      "val Loss: 4.3780 Acc: 0.9495\n",
      "Elapsed 10m 40s\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 13.8408 Acc: 0.9625\n",
      "val Loss: 4.3104 Acc: 0.9504\n",
      "Elapsed 11m 2s\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 13.7727 Acc: 0.9629\n",
      "val Loss: 4.2897 Acc: 0.9513\n",
      "Elapsed 11m 25s\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 13.5507 Acc: 0.9637\n",
      "val Loss: 4.2773 Acc: 0.9509\n",
      "Elapsed 11m 48s\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 13.4005 Acc: 0.9637\n",
      "val Loss: 4.2519 Acc: 0.9512\n",
      "Elapsed 12m 11s\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 13.4110 Acc: 0.9635\n",
      "val Loss: 4.3319 Acc: 0.9497\n",
      "Elapsed 12m 34s\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 13.7089 Acc: 0.9628\n",
      "val Loss: 4.2735 Acc: 0.9519\n",
      "Elapsed 12m 57s\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 13.3405 Acc: 0.9641\n",
      "val Loss: 4.2444 Acc: 0.9517\n",
      "Elapsed 13m 20s\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 13.0516 Acc: 0.9649\n",
      "val Loss: 4.2670 Acc: 0.9517\n",
      "Elapsed 13m 42s\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 12.9278 Acc: 0.9649\n",
      "val Loss: 4.2473 Acc: 0.9526\n",
      "Elapsed 14m 6s\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 12.7365 Acc: 0.9656\n",
      "val Loss: 4.2340 Acc: 0.9521\n",
      "Elapsed 14m 28s\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 12.6010 Acc: 0.9658\n",
      "val Loss: 4.2958 Acc: 0.9520\n",
      "Elapsed 14m 51s\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 12.5801 Acc: 0.9659\n",
      "val Loss: 4.3398 Acc: 0.9517\n",
      "Elapsed 15m 14s\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 12.4946 Acc: 0.9663\n",
      "val Loss: 4.3203 Acc: 0.9529\n",
      "Elapsed 15m 37s\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 12.3320 Acc: 0.9668\n",
      "val Loss: 4.4260 Acc: 0.9513\n",
      "Elapsed 15m 60s\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 12.2537 Acc: 0.9666\n",
      "val Loss: 4.3654 Acc: 0.9517\n",
      "Elapsed 16m 23s\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 12.1141 Acc: 0.9672\n",
      "val Loss: 4.2583 Acc: 0.9527\n",
      "Elapsed 16m 46s\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 12.1050 Acc: 0.9669\n",
      "val Loss: 4.3532 Acc: 0.9524\n",
      "Elapsed 17m 9s\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 12.3655 Acc: 0.9661\n",
      "val Loss: 4.3208 Acc: 0.9518\n",
      "Elapsed 17m 31s\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 12.1855 Acc: 0.9666\n",
      "val Loss: 4.3396 Acc: 0.9515\n",
      "Elapsed 17m 54s\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 12.5962 Acc: 0.9657\n",
      "val Loss: 4.5761 Acc: 0.9471\n",
      "Elapsed 18m 17s\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 12.5006 Acc: 0.9657\n",
      "val Loss: 4.3607 Acc: 0.9524\n",
      "Elapsed 18m 40s\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 11.8575 Acc: 0.9677\n",
      "val Loss: 4.3711 Acc: 0.9515\n",
      "Elapsed 19m 3s\n",
      "\n",
      "Training complete in 19m 3s\n",
      "Best val Acc: 0.952864\n"
     ]
    }
   ],
   "source": [
    "class Bilstm(nn.Module):\n",
    "    def __init__(self, embeddings, n_class, n_hidden):\n",
    "        super(Bilstm, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.Bilstm = nn.LSTM(input_size = embeddings.embedding_dim, hidden_size = n_hidden, bidirectional=True,\n",
    "                            num_layers=6)\n",
    "        self.fc = nn.Linear(n_hidden*2, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x, _ = self.Bilstm(x)\n",
    "        x = x.view(-1, x.shape[2])\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "n_batch = 150\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "model = Bilstm(embeddings, len(tags), 384)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 1e-3)\n",
    "model = trainModel(model,optimizer, dataset, n_batch,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE'] ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O'] ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predCache = []\n",
    "    \n",
    "\n",
    "for j in range(0,len(dataset['test']), n_batch):\n",
    "    z = encoding(dataset['test'][j:j+n_batch])\n",
    "    inputs, labels = zip(*z)\n",
    "    inputs = torch.stack(inputs)\n",
    "    labels = torch.stack(labels)\n",
    "    if use_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    preds, correct = predict(outputs, labels)\n",
    "    predCache.append(preds)\n",
    "\n",
    "yhat = torch.cat(predCache).cpu().numpy()\n",
    "yhat = [idx2tag[i] for i in yhat]\n",
    "y = [dataset['test'][i][1] for i in range(len(dataset['test']))]\n",
    "y = [j for i in y for j in i]\n",
    "x = [dataset['test'][i][0] for i in range(len(dataset['test']))]\n",
    "x = [j for i in x for j in i]\n",
    "\n",
    "print(x[:10],y[:10], yhat[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full report of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 socre: 0.620639\n",
      "Accuracy score: 0.935803\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      PER     0.5113    0.5745    0.5411      1617\n",
      "      ORG     0.5076    0.6237    0.5597      1661\n",
      "     MISC     0.5697    0.6752    0.6180       702\n",
      "      LOC     0.7029    0.8255    0.7593      1668\n",
      "\n",
      "micro avg     0.5739    0.6756    0.6206      5648\n",
      "macro avg     0.5741    0.6756    0.6206      5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"f1 socre: %f\"%(f1_score(y, yhat)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y, yhat)))\n",
    "\n",
    "report = classification_report(y, yhat,digits=4)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
