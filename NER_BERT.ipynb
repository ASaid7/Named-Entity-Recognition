{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:3em;\">**BERT IMPLIMENTATION**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_transformers import BertTokenizer, BertConfig\n",
    "from pytorch_transformers import BertForTokenClassification, AdamW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.expanduser('~'), 'Documents', 'AIT 726','HW3')\n",
    "\n",
    "\n",
    "def readfile(path):\n",
    "    f = open(path)\n",
    "    data = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]=='\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence,label))\n",
    "                sentence = []\n",
    "                label = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append(splits[0])\n",
    "        label.append(splits[-1][:-1])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        sentence = []\n",
    "        label = []\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "train = readfile(path + '/train.txt')\n",
    "val = readfile(path+'/valid.txt')\n",
    "test = readfile(path+'/test.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0,len:12\n",
      "texts:[CLS] EU rejects German call to boycott British la ##mb . [SEP]\n",
      "No.0,len:12\n",
      "lables:[CLS] B-ORG O B-MISC O O O B-MISC O X O [SEP]\n",
      "No.1,len:4\n",
      "texts:[CLS] Peter Blackburn [SEP]\n",
      "No.1,len:4\n",
      "lables:[CLS] B-PER I-PER [SEP]\n",
      "No.2,len:11\n",
      "texts:[CLS] BR ##US ##SE ##LS 1996 - 08 - 22 [SEP]\n",
      "No.2,len:11\n",
      "lables:[CLS] B-LOC X X X O X X X X [SEP]\n",
      "No.3,len:34\n",
      "texts:[CLS] The European Commission said on Thursday it disagreed with German advice to consumers to s ##hun British la ##mb until scientists determine whether mad cow disease can be transmitted to sheep . [SEP]\n",
      "No.3,len:34\n",
      "lables:[CLS] O B-ORG I-ORG O O O O O O B-MISC O O O O O X B-MISC O X O O O O O O O O O O O O O [SEP]\n",
      "No.4,len:39\n",
      "texts:[CLS] Germany ' s representative to the European Union ' s veterinary committee Werner Z ##wing ##mann said on Wednesday consumers should buy sheep ##me ##at from countries other than Britain until the scientific advice was clearer . [SEP]\n",
      "No.4,len:39\n",
      "lables:[CLS] B-LOC O X O O O B-ORG I-ORG O X O O B-PER I-PER X X O O O O O O O X X O O O O B-LOC O O O O O O O [SEP]\n",
      "[  101  7270 22961  1528  1840  1106 21423  1418  2495 12913   119   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0]\n",
      "[10  8  0  6  0  0  0  6  0  9  0 11  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#Special Tags for BERT X, [CLS], and [SEP]\n",
    "tags = ('O', 'I-LOC', 'B-PER', 'I-PER', 'I-ORG','I-MISC','B-MISC', 'B-LOC', 'B-ORG', 'X', '[CLS]','[SEP]')\n",
    "\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#set max length for bert\n",
    "max_len  = 45\n",
    "\n",
    "#Load Pretrained models tokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "tokenized_texts = []\n",
    "word_piece_labels = []\n",
    "i_inc = 0\n",
    "for word_list,label in train+val+test:\n",
    "    temp_lable = []\n",
    "    temp_token = []\n",
    "    \n",
    "    # Add [CLS] at the front \n",
    "    temp_lable.append('[CLS]')\n",
    "    temp_token.append('[CLS]')\n",
    "    \n",
    "    for word,lab in zip(word_list,label):\n",
    "        token_list = tokenizer.tokenize(word)\n",
    "        for m,token in enumerate(token_list):\n",
    "            temp_token.append(token)\n",
    "            if m==0:\n",
    "                temp_lable.append(lab)\n",
    "            else:\n",
    "                temp_lable.append('X')  \n",
    "                \n",
    "    # Add [SEP] at the end\n",
    "    temp_lable.append('[SEP]')\n",
    "    temp_token.append('[SEP]')\n",
    "    \n",
    "    tokenized_texts.append(temp_token)\n",
    "    word_piece_labels.append(temp_lable)\n",
    "    \n",
    "    if 5 > i_inc:\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_token)))\n",
    "        print(\"texts:%s\"%(\" \".join(temp_token)))\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_lable)))\n",
    "        print(\"lables:%s\"%(\" \".join(temp_lable)))\n",
    "    i_inc +=1\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(101)\n",
    "# Make text token into id\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(input_ids[0])\n",
    "\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels],\n",
    "                     maxlen=max_len, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "print(tags[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention Masks\n",
    "attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
    "\n",
    "#Train Val Test Split post encoding\n",
    "tr_inputs = input_ids[:14041]\n",
    "tr_tags = tags[:14041]\n",
    "tr_masks = attention_masks[:14041]\n",
    "tr_segs = segment_ids[:14041]\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "tr_segs = torch.tensor(tr_segs)\n",
    "\n",
    "\n",
    "val_inputs = input_ids[14041:17291]\n",
    "val_tags = tags[14041:17291]\n",
    "val_masks = attention_masks[14041:17291]\n",
    "val_segs = segment_ids[14041:17291]\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "val_segs = torch.tensor(val_segs)\n",
    "\n",
    "test_inputs = input_ids[17291:]\n",
    "test_tags = tags[17291:]\n",
    "test_masks = attention_masks[17291:]\n",
    "test_segs = segment_ids[17291:]\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_tags = torch.tensor(test_tags)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "test_segs = torch.tensor(test_segs)\n",
    "\n",
    "#Mini Batch Size\n",
    "batch_num = 32\n",
    "\n",
    "# Only set token embedding, attention embedding, no segment embedding\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Prep\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "#Load Pretrained Model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased',num_labels=len(tag2idx))\n",
    "\n",
    "# Set model to GPU,if you are using GPU machine\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs\n",
    "#Fine tuning the whole model\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 14041\n",
      "  Batch size = 32\n",
      "  Num steps = 1756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [01:36<04:49, 96.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.16124260212936903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [03:12<03:12, 96.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.027790374092260147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [04:48<01:36, 96.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01559402239025192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [06:25<00:00, 96.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.009839807140941787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Training model \n",
    "model.train()\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "        attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, scores = outputs[:2]\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "#model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "#output_model_file = os.path.join(path, \"Bert.pt\")\n",
    "#output_config_file = os.path.join(path, \"config.json\")\n",
    "\n",
    "#torch.save(model_to_save.state_dict(), output_model_file)\n",
    "#model_to_save.config.to_json_file(output_config_file)\n",
    "#tokenizer.save_vocabulary(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples =3250\n",
      "  Batch size = 32\n",
      "f1 socre: 0.936082\n",
      "Accuracy score: 0.988380\n",
      "***** Eval results *****\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      ORG     0.8898    0.9349    0.9118      1321\n",
      "      LOC     0.9742    0.9497    0.9618      1789\n",
      "      PER     0.9616    0.9700    0.9658      1731\n",
      "     MISC     0.8847    0.8983    0.8914       905\n",
      "\n",
      "micro avg     0.9280    0.9443    0.9361      5746\n",
      "macro avg     0.9369    0.9443    0.9404      5746\n",
      "\n",
      "f1 socre: 0.936082\n",
      "Accuracy score: 0.988380\n"
     ]
    }
   ],
   "source": [
    "#Validate Model\n",
    "model.eval();\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "    \n",
    "#     if step > 2:\n",
    "#         break\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask,)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "    \n",
    "    # Get NER predict result\n",
    "    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    #inputs to numpy\n",
    "    #input_ids = input_ids.to('cpu').numpy()\n",
    "    \n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    \n",
    "    \n",
    "    # Only predict the real word, mark=0, will not calculate\n",
    "    input_mask = input_mask.to('cpu').numpy()\n",
    "    \n",
    "    # Compare the valuable predict result\n",
    "    for i,mask in enumerate(input_mask):\n",
    "        # Real one\n",
    "        temp_1 = []\n",
    "        # Predict one\n",
    "        temp_2 = []\n",
    "        \n",
    "\n",
    "        \n",
    "        for j, m in enumerate(mask):\n",
    "            # Mark=0, meaning its a pad word, dont compare\n",
    "            if m:\n",
    "                if idx2tag[label_ids[i][j]] != \"X\" and idx2tag[label_ids[i][j]] != \"[CLS]\" and idx2tag[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n",
    "                    temp_1.append(idx2tag[label_ids[i][j]])\n",
    "                    temp_2.append(idx2tag[logits[i][j]])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "            \n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "        \n",
    "\n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "# Get acc , recall, F1 result report\n",
    "report = classification_report(y_true, y_pred,digits=4)\n",
    "print(\"***** Eval results *****\")\n",
    "print(\"\\n%s\"%(report))\n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples =3453\n",
      "  Batch size = 32\n",
      "f1 socre: 0.895597\n",
      "Accuracy score: 0.978935\n",
      "***** Eval results *****\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "     MISC     0.7216    0.8188    0.7671       690\n",
      "      LOC     0.9278    0.9204    0.9241      1620\n",
      "      PER     0.9555    0.9524    0.9540      1534\n",
      "      ORG     0.8675    0.8978    0.8824      1634\n",
      "\n",
      "micro avg     0.8818    0.9098    0.8956      5478\n",
      "macro avg     0.8916    0.9098    0.9003      5478\n",
      "\n",
      "f1 socre: 0.895597\n",
      "Accuracy score: 0.978935\n"
     ]
    }
   ],
   "source": [
    "#Test Model\n",
    "#model = BertForTokenClassification.from_pretrained('bert-base-cased',num_labels=len(tag2idx))\n",
    "#model.load_state_dict(torch.load(path+'/Bert.bin'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(test_inputs)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "    \n",
    "#     if step > 2:\n",
    "#         break\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask,)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "    \n",
    "    # Get NER predict result\n",
    "    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    #inputs to numpy\n",
    "    #input_ids = input_ids.to('cpu').numpy()\n",
    "    \n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    \n",
    "    \n",
    "    # Only predict the real word, mark=0, will not calculate\n",
    "    input_mask = input_mask.to('cpu').numpy()\n",
    "    \n",
    "    # Compare the valuable predict result\n",
    "    for i,mask in enumerate(input_mask):\n",
    "        # Real one\n",
    "        temp_1 = []\n",
    "        # Predict one\n",
    "        temp_2 = []\n",
    "        \n",
    "        \n",
    "        for j, m in enumerate(mask):\n",
    "            # Mark=0, meaning its a pad word, dont compare\n",
    "            if m:\n",
    "                if idx2tag[label_ids[i][j]] != \"X\" and idx2tag[label_ids[i][j]] != \"[CLS]\" and idx2tag[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n",
    "                    temp_1.append(idx2tag[label_ids[i][j]])\n",
    "                    temp_2.append(idx2tag[logits[i][j]])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "            \n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "        \n",
    "\n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "# Get acc , recall, F1 result report\n",
    "report = classification_report(y_true, y_pred,digits=4)\n",
    "print(\"***** Eval results *****\")\n",
    "print(\"\\n%s\"%(report))\n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
